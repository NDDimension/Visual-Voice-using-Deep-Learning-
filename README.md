# ğŸ‘„ Visible Voice â€“ Lip Reading AI

> ***See whatâ€™s being said â€” even in silence.***  



### âš ï¸ NOTE
This app is trained on controlled video datasets from **LipNet: End-to-End Sentence-level Lipreading** and is for **educational/demo use only**. It does not support real-time webcam input or noisy environments.


## ğŸ¥ App Preview

<p align="center">
  <img src="prev.png" width="400" alt="Lip Reading Demo Preview"/>
</p>

---

## ğŸ¯ Project Overview

**Visible Voice** is a computer vision + deep learning application that performs **lip reading**â€”converting silent video of a personâ€™s mouth into predicted text. It is ideal for assistive tech, silent communication, and low-audio environments.

> ğŸ§  Select a video of someone speaking from provided list. 
> ğŸ‘„ Model reads their lips 
> ğŸ’¬ Get back a sentence of what was said!

âœ… Real video frames
âœ… No audio required
âœ… Modern deep learning model
âœ… Interactive Streamlit app


## ğŸ“Œ Workflow

- Load & preprocess video frames

- Normalize grayscale mouth regions

- Parse ground truth alignment tokens

- Tokenize text with character-level mapping

- Build deep 3D CNN + BiLSTM model

- Train on sentence alignment data

- Streamlit app for visual prediction & decoding

---

## ğŸ§  How It Works

1. **Video Preprocessing:** Extracts mouth-region grayscale frames from each video

2. **Text Alignment:** Loads phoneme-to-text mappings and converts to character tokens

3. **Model Architecture:**

    - 3D Convolutions to capture spatiotemporal features

    - Bi-LSTM to capture context across frames

    - CTC Loss to handle unaligned sequences

4. **Prediction Output:** Model predicts tokens, which are decoded back into text

---

## ğŸ” Features

| Function               | Description                                          |
| ---------------------- | ---------------------------------------------------- |
| ğŸ¥ Video Input         | Upload pre-recorded videos for lip analysis          |
| ğŸï¸ Frame Extraction   | Converts video to frame sequence of mouth region     |
| ğŸ§  Deep Neural Network | CNN + Bi-LSTM model for sequence-to-text prediction  |
| ğŸ”¡ Character Decoder   | Converts predicted tokens into readable text         |
| ğŸ–¥ï¸ Clean Streamlit UI | Sleek web interface with prediction highlights       |
| ğŸ” CTC Decoding        | Handles variable-length input/output alignment       |
| ğŸ“¦ Pretrained Weights  | Model loads from saved checkpoints for quick testing |

---

## ğŸ§ª Model Architecture

- **Conv3D Layers:** Capture spatial + temporal mouth movements

- **MaxPooling:** Reduces resolution while keeping features

- **TimeDistributed Flattening:** Prepares 3D frames for sequence modeling

- **Bidirectional LSTM:** Learns time-sequential speech patterns

- **Dense Softmax Output:** Character-level probability output

- **CTC Loss:** Enables training without frame-level text alignment


## ğŸ“¦ Requirements

```txt
opencv-python==4.6.0.66
matplotlib==3.6.2
imageio==2.23.0
gdown==4.6.0
tensorflow==2.15.0
streamlit
etc
```

Install everything using:
```bash
pip install -r requirements.txt
```

You need to download **ffmpeg** also as to run videos and convert them to `.mp4`.
> Link : https://ffmpeg.org/download.html ( download according to your os )


## âš™ï¸ Getting Started

1ï¸âƒ£ **Clone the repository**
```bash
git clone https://github.com/yourusername/Lip-Reading-AI.git
cd Lip-Reading-AI
```

2ï¸âƒ£ **Download and prepare your dataset**
- Download dataset (e.g., GRID corpus or similar) or you can simply download it from given link

> Dataset : https://drive.google.com/uc?id=1YlvpDLix3S-U8fd-gqRwPcWXAXm8JwjL

- Place .mpg videos in: data/s1/

- Place .align label files in: data/alignments/s1/

3ï¸âƒ£ **Train the Model (optional)**

You can either train your model and save it or else can use the `checkpoint.zip` to load and use model.
```
Run: LipNet_Refined.ipynb
```
Will generate weights and store them in`/models/checkpoint`

4ï¸âƒ£ **Run the Streamlit App**
```bash
cd app
streamlit run app.py
```

5ï¸âƒ£ **Use the App**

- Select a video from list and see the magic!

- View token and predicted sentence ; also can confirm that by playing the video .



## âœ¨ Highlights

- âœ… End-to-end lip reading from video

- âœ… Uses real alignment data

- âœ… Clean & informative UI

- âœ… Model accuracy viewable via token prediction

- âœ… Easily extendable to real-time or audio-paired models



## ğŸ”® Future Improvements
- ğŸ§‘â€ğŸ’» Real-time webcam lip reading

- ğŸ§  Transformer-based models (e.g., Lipformer)

- ğŸ—£ï¸ Audio-visual fusion models

- ğŸŒ Multilingual lip reading

- ğŸ“± Responsive mobile interface


## ğŸ“Š Sample Output

| Video File   | Predicted Sentence           | Accuracy (Est.) |
| ------------ | ---------------------------- | --------------- |
| `bbaf2n.mpg` | â€œbin blue at f two nowâ€      | 100%             |
| `bbbm1s.mpg` | â€œbin blue by m one soonâ€ | 100%             |


### ğŸ§  Powered by:

- TensorFlow / Keras

- OpenCV

- Streamlit

- NumPy

- ImageIO

- GDown



## ğŸ“œ License

Licensed under the [MIT License](LICENSE).



> **Visible Voice** â€” *Empowering speech through vision.*

> â¤ï¸ Made with love by **Dhanraj Sharma**.
